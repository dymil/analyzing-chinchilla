{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REwsunoSi2qZ"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1VjyCne7IYPr"
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import warnings\n",
    "# from os import CLD_CONTINUED\n",
    "from itertools import product\n",
    "\n",
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit, basinhopping, OptimizeWarning\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics._plot.confusion_matrix import unique_labels\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "np.seterr(over='ignore')\n",
    "np.seterr(invalid='ignore')\n",
    "warnings.filterwarnings(\"ignore\", category=OptimizeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9613JmZi4rL"
   },
   "source": [
    "# Loading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xNmzz51Syr3y"
   },
   "outputs": [],
   "source": [
    "# Use pandas to read the CSV file directly from the URL\n",
    "training_df = pd.read_csv('data/svg_extracted_data.csv')\n",
    "\n",
    "# The DataFrame 'training_df' now contains the data from the CSV file\n",
    "\n",
    "training_df['Training Tokens'] = training_df['Training FLOP']/(6.0*training_df['Model Size'])\n",
    "training_df = training_df[['Model Size', 'Training Tokens', 'Training FLOP', 'loss']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bIDfE715zshD",
    "outputId": "61a060a5-4b8a-4701-c3a0-dd16c4a3ef11"
   },
   "outputs": [],
   "source": [
    "training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jhg91I1IGMt-",
    "outputId": "053138d5-6852-4736-9600-dc3ff93a590d"
   },
   "outputs": [],
   "source": [
    "training_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMfneQcMafzf"
   },
   "source": [
    "# Plot of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cQGOt60jahp9",
    "outputId": "d1dd6c8a-9d41-4029-a35d-8d30a51c9342"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Adjusting default font sizes for better readability\n",
    "plt.rcParams.update({'font.size': 12, 'axes.labelsize': 14, 'axes.titlesize': 16})\n",
    "\n",
    "# Creating the scatter plot\n",
    "plt.figure(figsize=(10/1.3, 6/1.3))\n",
    "sc = plt.scatter(training_df[\"Training FLOP\"], training_df[\"Model Size\"],\n",
    "                 c=training_df[\"loss\"], cmap='magma', norm=LogNorm(vmin=2.0, vmax=5.0), marker='o', edgecolors='black', linewidths=0.5)\n",
    "\n",
    "# Adding a color bar\n",
    "cbar = plt.colorbar(sc)\n",
    "cbar.set_label('Loss', rotation=270, labelpad=15)\n",
    "\n",
    "# Directly setting color bar tick labels to avoid scientific notation\n",
    "cbar_ticks = [2, 3, 4, 5]  # Define the ticks you want based on your data range\n",
    "cbar.set_ticks(cbar_ticks)  # Set these ticks on the color bar\n",
    "cbar.set_ticklabels([str(tick) for tick in cbar_ticks])  # Set the tick labels as plain strings of these ticks\n",
    "\n",
    "# Setting the plot labels and title\n",
    "plt.xlabel('Training FLOP')\n",
    "plt.ylabel('Model Size')\n",
    "\n",
    "# Applying log scale to both axes\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "# Custom formatter function for y-axis\n",
    "def custom_formatter(x, pos):\n",
    "    if x >= 1e9:\n",
    "        return '{:d}B'.format(int(x/1e9))\n",
    "    elif x >= 1e6:\n",
    "        return '{:d}M'.format(int(x/1e6))\n",
    "    elif x >= 1e3:\n",
    "        return '{:d}K'.format(int(x/1e3))\n",
    "    else:\n",
    "        return str(int(x))\n",
    "\n",
    "# Applying the custom formatter to the y-axis\n",
    "plt.gca().yaxis.set_major_formatter(ticker.FuncFormatter(custom_formatter))\n",
    "\n",
    "# Custom formatter for axes to display ticks as 10^x\n",
    "plt.gca().xaxis.set_major_formatter(ticker.LogFormatterSciNotation(base=10, labelOnlyBase=False))\n",
    "\n",
    "# Setting axes limits\n",
    "plt.xlim([min(training_df['Training FLOP'])*0.85, max(training_df['Training FLOP'])*1.15])\n",
    "plt.ylim([min(training_df['Model Size'])*0.85, max(training_df['Model Size'])*1.15])\n",
    "\n",
    "# Saving the plot to a PDF\n",
    "plt.savefig('training_flop_vs_model_size.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rgJu20jHniL9"
   },
   "outputs": [],
   "source": [
    "# Custom formatter function for y-axis\n",
    "def custom_formatter(x, pos):\n",
    "    if x >= 1e9:\n",
    "        return '{:d}B'.format(int(x/1e9))\n",
    "    elif x >= 1e6:\n",
    "        return '{:d}M'.format(int(x/1e6))\n",
    "    elif x >= 1e3:\n",
    "        return '{:d}K'.format(int(x/1e3))\n",
    "    else:\n",
    "        return str(int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6jAZV9WsnEJN",
    "outputId": "15e36110-b02c-4a11-ea7b-4a6d04b3f22a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.transforms import Bbox\n",
    "\n",
    "# Adjusting default font sizes for better readability\n",
    "plt.rcParams.update({'font.size': 12, 'axes.labelsize': 14, 'axes.titlesize': 16})\n",
    "\n",
    "# Creating the scatter plot\n",
    "fig, ax = plt.subplots(figsize=(12/1.2, 6/1.2))\n",
    "sc = ax.scatter(training_df[\"Training FLOP\"], training_df[\"Model Size\"],\n",
    "                c=training_df[\"loss\"], cmap='magma', norm=LogNorm(vmin=1.804501, vmax=5.0), marker='o', edgecolors='black', linewidths=0.5)\n",
    "\n",
    "# Adding a color bar on the right, but with ticks and text to the left of the bar\n",
    "cbar = fig.colorbar(sc, ax=ax, location='right', pad=0.15)\n",
    "cbar.set_label('Loss', rotation=90, labelpad=15)\n",
    "cbar.ax.yaxis.set_ticks_position('left')\n",
    "cbar.ax.yaxis.set_label_position('left')\n",
    "\n",
    "# Adjusting subplot parameters to remove whitespace\n",
    "fig.subplots_adjust(left=0.1, right=0.8)  # Adjust the right margin here\n",
    "\n",
    "# Removing the top and right spines\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Directly setting color bar tick labels to avoid scientific notation\n",
    "cbar_ticks = [2.00, 3.00, 4.00, 5.00]\n",
    "cbar.set_ticks(cbar_ticks)\n",
    "cbar.set_ticklabels(['{:.2f}'.format(tick) for tick in cbar_ticks])\n",
    "\n",
    "# Setting the plot labels and title\n",
    "ax.set_xlabel('Training FLOP')\n",
    "ax.set_ylabel('Model Size')\n",
    "\n",
    "# Applying log scale to both axes\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# Custom formatter for axes\n",
    "ax.xaxis.set_major_formatter(ticker.LogFormatterSciNotation(base=10, labelOnlyBase=False))\n",
    "\n",
    "# Setting axes limits\n",
    "ax.set_xlim([min(training_df['Training FLOP'])*0.85, max(training_df['Training FLOP'])*1.15])\n",
    "ax.set_ylim([min(training_df['Model Size'])*0.85, max(training_df['Model Size'])*1.15])\n",
    "\n",
    "# Saving the plot to a PDF\n",
    "# Get the current bounding box of the figure\n",
    "fig_bbox = fig.get_tightbbox(fig.canvas.get_renderer())\n",
    "\n",
    "# Create a new bounding box by modifying the original\n",
    "# Increase left and right margins by specifying a shift\n",
    "padding = 0.85 # Change the padding value as needed (in inches)\n",
    "new_fig_bbox = Bbox.from_extents(\n",
    "    fig_bbox.x0 - padding,  # Extend left by padding inches\n",
    "    fig_bbox.y0,            # Keep bottom the same\n",
    "    fig_bbox.x1 + padding+0.25,  # Extend right by padding inches\n",
    "    fig_bbox.y1             # Keep top the same\n",
    ")\n",
    "\n",
    "# Save the figure using the new bounding box\n",
    "plt.savefig('training_flop_vs_model_size.pdf', bbox_inches=new_fig_bbox)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zcKLy19EoKcy"
   },
   "outputs": [],
   "source": [
    "# outlier datapoints\n",
    "training_df['d_n_ratio'] = training_df['Training Tokens']/training_df['Model Size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qlE7ErU7e4QH",
    "outputId": "95009a7d-026e-4ef5-a2b2-8e58fc493578"
   },
   "outputs": [],
   "source": [
    "training_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DvHelBaIe01"
   },
   "source": [
    "# Replicate methodology from Chinchilla paper\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqw8ksKqjRqc"
   },
   "source": [
    "Define things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGlxqx3CZpTz"
   },
   "outputs": [],
   "source": [
    "nr_of_models_excluded = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IH19dIBopNBN"
   },
   "outputs": [],
   "source": [
    "N = training_df['Model Size'].values\n",
    "D = training_df['Training Tokens'].values\n",
    "losses = training_df['loss'].values\n",
    "bootstraps = 200\n",
    "\n",
    "sorted_losses = sorted(losses)\n",
    "if nr_of_models_excluded == 0:\n",
    "    indices = list(range(len(N)))\n",
    "else:\n",
    "    sorted_losses = sorted(losses)\n",
    "    indices = [i for i in range(len(N)) if losses[i] < sorted_losses[-nr_of_models_excluded]]\n",
    "\n",
    "np.random.seed(42)\n",
    "random_indices = [np.random.choice(indices, size=len(indices), replace=True) for _ in range(bootstraps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NwwVqy2CZ4Bq"
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd.scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import erf, logsumexp\n",
    "\n",
    "# true_params = np.array([np.log(406.4), np.log(410.7), np.log(1.69), 0.34, 0.28])\n",
    "# A, B, E, alpha, beta?\n",
    "true_params = np.array([6.0073404, 6.0179186, 0.5267228, 0.33917084, 0.2849083])\n",
    "true_params_rounded = np.array([np.log(406.4), np.log(410.7), np.log(1.69), 0.34, 0.28])\n",
    "\n",
    "# Define the log-sum-exp function\n",
    "def log_sum_exp(a, b, e, alpha, beta, N: NDArray, D: NDArray):\n",
    "    # print(a,b,e,alpha,beta,N,D)\n",
    "    # return logsumexp(np.stack((a - alpha * np.log(N), b - beta * np.log(D), np.tile(e, N.shape[0]))), axis=0)\n",
    "    return np.log(np.exp(a - alpha * np.log(N)) + np.exp(b - beta * np.log(D)) + np.exp(e))\n",
    "\n",
    "# Define the Huber loss function\n",
    "def custom_huber_loss(y_true, y_pred, delta=1e-3):\n",
    "    # Calculate the difference\n",
    "    diff = y_true - y_pred\n",
    "    # Calculate the condition for Huber loss\n",
    "    cond = np.abs(diff) <= delta\n",
    "    # Apply Huber loss formula\n",
    "    loss = np.where(cond, 0.5 * diff**2, delta * (np.abs(diff) - 0.5 * delta))\n",
    "    return np.sum(loss)\n",
    "\n",
    "def huber_normalizing_factor(delta=1e-3):\n",
    "    return np.sqrt(2*np.pi) * (1 - 2*norm.sf(delta)) + 2 * np.exp(-0.5*delta**2)/delta\n",
    "\n",
    "def huber_logpdf(x, delta=1e-3, loc=0, scale=1):\n",
    "    x = (x-loc)/scale\n",
    "\n",
    "    cond = np.abs(x) <= delta\n",
    "    loss = np.where(cond, 0.5 * x**2, delta * (np.abs(x) - 0.5 * delta))\n",
    "    return -loss - np.log(huber_normalizing_factor(delta=delta)) - np.log(scale)\n",
    "\n",
    "def huber_pdf(x, delta=1e-3, loc=0, scale=1):\n",
    "    return np.exp(huber_logpdf(x, delta=delta, loc=loc, scale=scale))\n",
    "\n",
    "# Define the objective function to be minimized\n",
    "def objective(params, N, D, losses):\n",
    "    a, b, e, alpha, beta, sigma = params\n",
    "    predictions = log_sum_exp(a, b, e, alpha, beta, N, D)\n",
    "    return -np.sum(huber_logpdf(np.log(losses), loc=predictions, scale=np.exp(sigma), delta=1e-3))\n",
    "    # return custom_huber_loss(np.log(losses), predictions, delta=1e-3)\n",
    "\n",
    "def scale_objective(sigma, params, N, D, losses):\n",
    "    a, b, e, alpha, beta = params\n",
    "    predictions = log_sum_exp(a, b, e, alpha, beta, N, D)\n",
    "    return -np.sum(huber_logpdf(np.log(losses), loc=predictions, scale=np.exp(sigma), delta=1e-3))\n",
    "    # return custom_huber_loss(np.log(losses), predictions, delta=1e-3)\n",
    "\n",
    "def constant_term_objective(params, a, b, alpha, beta, N, D, losses):\n",
    "    e, sigma = params\n",
    "    predictions = log_sum_exp(a, b, e, alpha, beta, N, D)\n",
    "    return -np.sum(huber_logpdf(np.log(losses), loc=predictions, scale=np.exp(sigma), delta=1e-3))\n",
    "\n",
    "def huber_loss_objective(params, N, D, losses):\n",
    "    a, b, e, alpha, beta = params\n",
    "    predictions = log_sum_exp(a, b, e, alpha, beta, N, D)\n",
    "    return custom_huber_loss(np.log(losses), predictions, delta=1e-3)\n",
    "\n",
    "# Define the parameter untransform\n",
    "def untransform_params(param_array):\n",
    "    if len(np.shape(param_array)) == 2:\n",
    "      return np.hstack((np.exp(param_array[:, :3]), param_array[:, 3:]))\n",
    "    else:\n",
    "      return np.hstack((np.exp(param_array[:3]), param_array[3:]))\n",
    "\n",
    "# Define the Huber loss function on residuals\n",
    "def huber_loss(residuals, delta=1e-3):\n",
    "    # Calculate the difference\n",
    "    diff = residuals\n",
    "    # Calculate the condition for Huber loss\n",
    "    cond = np.abs(diff) <= delta\n",
    "    # Apply Huber loss formula\n",
    "    loss = np.where(cond, 0.5 * diff**2, delta * (np.abs(diff) - 0.5 * delta))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMZk-u0p35fF"
   },
   "source": [
    "### Replicate exactly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "XDTSETrS3hMK",
    "outputId": "48baea41-7f2d-4c96-c974-859c81a230fb"
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "N = training_df['Model Size'].values\n",
    "D = training_df['Training Tokens'].values\n",
    "losses = training_df['loss'].values\n",
    "\n",
    "# Set up the grid for initial parameter values\n",
    "alpha_vals = np.arange(0, 2.5, 0.5)\n",
    "beta_vals = np.arange(0, 2.5, 0.5)\n",
    "e_vals = np.arange(-1, 1.5, 0.5)\n",
    "a_vals = np.arange(0, 30, 5)\n",
    "b_vals = np.arange(0, 30, 5)\n",
    "\n",
    "# Perform the optimization using L-BFGS over the grid of initial values\n",
    "best_loss = np.inf\n",
    "best_params = None\n",
    "\n",
    "from itertools import product\n",
    "results_dict = {}\n",
    "for alpha, beta, e, a, b in (tuple(x.item() for x in v) for v in product(alpha_vals, beta_vals, e_vals, a_vals, b_vals)):\n",
    "    init_params = [a, b, e, alpha, beta]\n",
    "    result = minimize(huber_loss_objective, init_params, args=(N[indices], D[indices], losses[indices]), method='L-BFGS-B')\n",
    "    results_dict[tuple(init_params)] = {'params': result.x, 'loss': result.fun}\n",
    "    if result.success and result.fun < best_loss:\n",
    "        best_loss = result.fun\n",
    "        best_params = result.x\n",
    "        print(f\"New best loss: {best_loss}\")\n",
    "        print(f\"Best params: {best_params}\")\n",
    "        print(f\"Initial guess: {init_params}\")\n",
    "\n",
    "# Transform the fitted parameters a, b, e to A, B, E\n",
    "if best_params is not None:\n",
    "    A = np.exp(best_params[0])\n",
    "    B = np.exp(best_params[1])\n",
    "    E = np.exp(best_params[2])\n",
    "    alpha = best_params[3]\n",
    "    beta = best_params[4]\n",
    "    print(f\"Best loss: {best_loss}, best fit parameters: A={A}, B={B}, E={E}, alpha={alpha}, beta={beta}\")\n",
    "else:\n",
    "    print(\"Optimization failed to converge.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vnKUy1TkA_z"
   },
   "source": [
    "# Chinchilla parametric fit fits the data poorly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzvHqWMEBxNq"
   },
   "source": [
    "### Our estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "oLoFowoEIoV1",
    "outputId": "a7095f08-062c-4370-9da9-7274c234b295"
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "\n",
    "# Set up the grid for initial parameter values\n",
    "param_list = []\n",
    "param_list_grid = []\n",
    "jac = grad(huber_loss_objective)\n",
    "\n",
    "for num, indices in enumerate(random_indices):\n",
    "  # Perform the optimization using BFGS\n",
    "  best_loss = np.inf\n",
    "  best_params = None\n",
    "\n",
    "  init_params = true_params\n",
    "  result = minimize(huber_loss_objective, init_params, args=(N[indices], D[indices], losses[indices]), \\\n",
    "                    jac=jac, method='BFGS')\n",
    "  param_list.append(result.x)\n",
    "\n",
    "  best_loss = result.fun\n",
    "  best_params = result.x\n",
    "  #print(f\"New best loss: {best_loss}\")\n",
    "  #print(f\"Best params: {best_params}\")\n",
    "\n",
    "  init_params = list(map(lambda vals: np.random.choice(vals), [a_vals, b_vals, e_vals, alpha_vals, beta_vals]))\n",
    "  result = minimize(huber_loss_objective, init_params, args=(N[indices], D[indices], losses[indices]), \\\n",
    "                    jac=jac, method='BFGS')\n",
    "  param_list_grid.append(result.x)\n",
    "  \n",
    "  if num % 100 == 99:\n",
    "    print(\"Bootstrap step %d completed\" % (num+1))\n",
    "\n",
    "param_list = np.array(param_list)\n",
    "cov_matrix = np.cov(np.transpose(param_list))\n",
    "\n",
    "param_list_grid = np.array(param_list_grid)\n",
    "cov_matrix_grid = np.cov(np.transpose(param_list_grid))\n",
    "param_list_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "arjAyJkhGuWu",
    "outputId": "5f2f2bb9-7a2a-44f9-fa5b-e75bc2c67053"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Applying the given transformations\n",
    "transformed_params = np.exp(param_list[:, :3])  # Apply exp() to the first three parameters\n",
    "alpha_beta = param_list[:, 3:]  # The last two parameters remain unchanged\n",
    "transformed_params = np.hstack([transformed_params, alpha_beta])  # Combine the transformed and untransformed parameters\n",
    "\n",
    "transformed_params_grid = np.exp(param_list_grid[:, :3])  # Apply exp() to the first three parameters\n",
    "transformed_params_grid = np.hstack([transformed_params_grid, param_list_grid[:, 3:]])  # Combine the transformed and untransformed parameters\n",
    "\n",
    "# Creating a DataFrame for the transformed parameters\n",
    "transformed_params_df = pd.DataFrame(\n",
    "    transformed_params,\n",
    "    columns=['A', 'B', 'E', r'$\\alpha$', r'$\\beta$']\n",
    ")\n",
    "\n",
    "transformed_params_grid_df = pd.DataFrame(\n",
    "    transformed_params_grid,\n",
    "    columns=['A', 'B', 'E', r'$\\alpha$', r'$\\beta$']\n",
    ")\n",
    "\n",
    "pd.DataFrame(transformed_params_grid, columns=['A', 'B', 'E', r'$\\alpha$', r'$\\beta$']).assign(setting='grid')\n",
    "np.hstack([np.exp([A,B,E]), [alpha, beta]])\n",
    "big_df = pd.concat([transformed_params_grid_df.assign(setting='grid'),\n",
    "                                   transformed_params_df.assign(setting='og'),\n",
    "                                #    pd.DataFrame(transformed_params_grid, columns=['A', 'B', 'E', r'$\\alpha$', r'$\\beta$']).assign(setting='grid'),\n",
    "                                   pd.DataFrame(np.array([[A,B,E,alpha, beta]]), columns=['A', 'B', 'E', r'$\\alpha$', r'$\\beta$']).assign(setting='best'),\n",
    "                                   pd.DataFrame(np.hstack([np.exp(true_params[:3]), true_params[3:]]).reshape(1, -1), columns=['A', 'B', 'E', r'$\\alpha$', r'$\\beta$']).assign(setting='true'),#])\n",
    "                                   ])\n",
    "\n",
    "# Plotting the distribution of each transformed parameter across bootstraps\n",
    "grid = sns.pairplot(big_df, diag_kind='kde', corner=True, hue='setting')#, kind='kde')\n",
    "print(true_params)\n",
    "print(pd.DataFrame(np.hstack([np.exp(true_params[:3]), true_params[3:]]).reshape(1, -1), columns=['A', 'B', 'E', r'$\\alpha$', r'$\\beta$']))\n",
    "plt.suptitle('Distribution of Transformed Parameters Across Bootstraps', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "oXSCOQ4rIuIn"
   },
   "outputs": [],
   "source": [
    "param_list = np.array(param_list)\n",
    "cov_matrix = np.cov(np.transpose(param_list))\n",
    "\n",
    "param_list_untransformed = untransform_params(param_list)\n",
    "cov_matrix_untransformed = np.cov(np.transpose(param_list_untransformed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Pyi4B06nLKpT",
    "outputId": "352ff78a-b7af-4a01-cfc1-b3e6f1019faf"
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "\n",
    "init_params = list(true_params) + [0]\n",
    "\n",
    "indices = list(range(len(N))) if nr_of_models_excluded == 0 else [i for i in range(len(N)) if losses[i] < sorted(losses)[-nr_of_models_excluded]]\n",
    "\n",
    "result = minimize(objective, init_params, args=(N[indices], D[indices], losses[indices]), method='BFGS',\n",
    "                  jac=grad(objective))\n",
    "\n",
    "print(result)\n",
    "print(result.x)\n",
    "\n",
    "estimated_params = result.x[:5]\n",
    "estimated_params_untransformed = untransform_params(estimated_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "oIE3ieKTFO33",
    "outputId": "af7f27a6-132b-4ff2-d6d6-3c83988bd65e"
   },
   "outputs": [],
   "source": [
    "standard_errors = np.sqrt(np.diag(cov_matrix[:5, :5]))\n",
    "standard_errors_untransformed = np.sqrt(np.diag(cov_matrix_untransformed[:5, :5]))\n",
    "\n",
    "parameter_labels = [\"A\", \"B\", \"E\", \"alpha\", \"beta\"]\n",
    "print(\"Parameter estimates and their standard errors\\n\")\n",
    "for index, label in enumerate(parameter_labels):\n",
    "  print(\"%s: %.5f (%.5f)\" % (label, estimated_params_untransformed[index], standard_errors_untransformed[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "uJBlhbM_PkII",
    "outputId": "cbe3fae8-b3ac-40f8-a907-e698c426bd33"
   },
   "outputs": [],
   "source": [
    "# Calculating 95% Confidence Intervals for each parameter\n",
    "confidence_intervals = {}\n",
    "\n",
    "# For each column in the DataFrame, calculate the 2.5th and 97.5th percentiles\n",
    "for column in transformed_params_df.columns:\n",
    "    lower_bound = np.percentile(transformed_params_df[column], 2.5)\n",
    "    upper_bound = np.percentile(transformed_params_df[column], 97.5)\n",
    "    confidence_intervals[column] = (lower_bound, upper_bound)\n",
    "\n",
    "# Printing out the 95% Confidence Intervals for each parameter\n",
    "print(\"95% Confidence Intervals for Parameter Estimates\\n\")\n",
    "for parameter, (lower, upper) in confidence_intervals.items():\n",
    "    print(f\"{parameter}: ({lower:.3f}, {upper:.3f})\")\n",
    "\n",
    "for column in transformed_params_df.columns:\n",
    "    lower_bound = np.percentile(transformed_params_df[column], 2.5)\n",
    "    upper_bound = np.percentile(transformed_params_df[column], 97.5)\n",
    "    confidence_intervals[column] = (lower_bound, upper_bound)\n",
    "\n",
    "# Printing out the 95% Confidence Intervals for each parameter\n",
    "print(\"95% Confidence Intervals for Parameter Estimates\\n\")\n",
    "for parameter, (lower, upper) in confidence_intervals.items():\n",
    "    print(f\"{parameter}: ({lower:.3f}, {upper:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "x3mg5SXPPj_O"
   },
   "outputs": [],
   "source": [
    "true_params_unlogged = np.array([np.exp(6.0073404), np.exp(6.0179186), np.exp(0.5267228), 0.33917084, 0.2849083])\n",
    "true_params_rounded_unlogged = np.array([406.4, 410.7, 1.69, 0.34, 0.28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "X_uTM9x3QUtz",
    "outputId": "a11ccf0b-5166-4a5b-9819-ad702b05c987"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import t\n",
    "import numpy as np\n",
    "\n",
    "# Calculate t-statistics\n",
    "t_statistics = (estimated_params_untransformed - true_params_unlogged) / standard_errors_untransformed\n",
    "\n",
    "# Degrees of freedom\n",
    "degrees_of_freedom = len(indices) -5\n",
    "\n",
    "# Calculate two-tailed p-values\n",
    "p_values = t.sf(np.abs(t_statistics), degrees_of_freedom) * 2  # times 2 for two-tailed test\n",
    "\n",
    "# Print parameter names alongside p-values\n",
    "for label, p_value in zip(parameter_labels, p_values):\n",
    "    print(f\"{label}: P-value = {p_value:.1e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsNvcJNNNx9W"
   },
   "source": [
    "### Chi squared test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "EbbOfnf2cklW",
    "outputId": "4e513392-51e1-4bfc-f593-1613f73ac3a3"
   },
   "outputs": [],
   "source": [
    "transformed_params_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFzGcucEPJOp"
   },
   "source": [
    "Chi squared for equality of all parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "rRICy6MHc9i4",
    "outputId": "8f351470-225e-462e-a3a9-33c133a904aa"
   },
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "DppoGlZ7PjKe"
   },
   "outputs": [],
   "source": [
    "#baseline_fitted_params = [  6.17795598,   7.64273236,   0.59711196,   0.34781303,   0.36585412  ]\n",
    "\n",
    "baseline_fitted_params = [  6.17795598,   7.64273236,   0.59711196,   0.34781303,   0.36585412  ]\n",
    "\n",
    "#fitted_params_with_outliers = [6.13834459, 9.43584213, 0.63412782, 0.3453568 , 0.45189211]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "VQF3lQH3Nr3_",
    "outputId": "3125e762-7281-4588-a820-ec82635aa70c"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "import numpy as np\n",
    "\n",
    "def chi_squared_stat(params_1, params_2, cov_matrix):\n",
    "  return np.transpose(params_1 - params_2) @ np.linalg.inv(cov_matrix) @ (params_1 - params_2)\n",
    "\n",
    "print(\"Difference between Hoffmann et al. (2022) params and our params:\", true_params - baseline_fitted_params)\n",
    "chi_squared = chi_squared_stat(true_params, estimated_params, cov_matrix[:5, :5])\n",
    "\n",
    "print(\"Implied chi^2 (df=5) test statistic: %.2f\" % (chi_squared))\n",
    "print(\"Implied chi^2 (df=5) p-value: %.2e\\n\" % (chi2.sf(chi_squared, df=5)))\n",
    "\n",
    "print(\"Difference between our default (excluding five outliers) params and our params without any outlier exclusion:\", baseline_fitted_params - estimated_params)\n",
    "chi_squared = chi_squared_stat(baseline_fitted_params, estimated_params, cov_matrix[:5, :5])\n",
    "\n",
    "print(\"Implied chi^2 (df=5) test statistic: %.2f\" % (chi_squared))\n",
    "print(\"Implied chi^2 (df=5) p-value: %.2e\" % (chi2.sf(chi_squared, df=5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zl6peVA6FPcN"
   },
   "source": [
    "# Our fit is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qn-j1KQgsRhR"
   },
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "9CMo-v4cjy0s"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def scaling_law(N, D, params):\n",
    "    logA, logB, logE, alpha, beta = params\n",
    "    A, B, E = np.exp([logA, logB, logE])\n",
    "    return E + A/N**alpha + B/D**beta\n",
    "\n",
    "# Your residuals calculation\n",
    "A, B, E, alpha, beta = true_params_unlogged\n",
    "residuals = losses[indices] - scaling_law(N[indices], D[indices], true_params_rounded)  # Hoffmann estimates\n",
    "mse = np.mean(residuals**2)\n",
    "\n",
    "residuals_ours = losses[indices] - np.exp(log_sum_exp(*estimated_params, N[indices], D[indices]))\n",
    "mse_ours = np.mean(residuals_ours**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "EltpngOlSLUy",
    "outputId": "7b75deb3-821b-4bc9-ccc2-6d11b1924683"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.path import Path\n",
    "\n",
    "# Assuming residuals_2 and residuals_2_ours are numpy arrays with your data\n",
    "# Define the data\n",
    "\n",
    "color_ours = 0.35\n",
    "color_chinchilla = 0.65\n",
    "\n",
    "data_control = residuals  # replace with your actual data for population-matched controls\n",
    "data_active = residuals_ours  # replace with your actual data for musically active cases\n",
    "\n",
    "# Create a new figure\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "# Check for the largest residual and set the y-limit if necessary\n",
    "largest_residual = max(np.max(data_control), np.max(data_active))\n",
    "if largest_residual > 0.6:\n",
    "    ax.set_ylim(-0.15, 0.2)\n",
    "\n",
    "# Scatter plots with a small random noise to x-values for jitter effect\n",
    "scatter_control = np.random.normal(1, 0.04, size=len(data_control))\n",
    "scatter_active = np.random.normal(2, 0.04, size=len(data_active))\n",
    "ax.scatter(scatter_control, data_control, alpha=0.25, color=plt.cm.viridis(color_chinchilla))\n",
    "ax.scatter(scatter_active, data_active, alpha=0.25, color=plt.cm.viridis(color_ours))\n",
    "\n",
    "# Offset for the violin plots to be right of the scatter points\n",
    "violin_offset = 0.1  # Adjust as needed\n",
    "\n",
    "# Create violin plots on the same axis as scatter plots, slightly offset to the right\n",
    "violin_parts = ax.violinplot([data_control, data_active], positions=[1 + violin_offset, 2 + violin_offset],\n",
    "                             widths=0.8, showmeans=False, showextrema=False, showmedians=True)\n",
    "\n",
    "# Make the violin plot one-sided by adjusting its paths\n",
    "colors = [plt.cm.viridis(color_chinchilla), plt.cm.viridis(color_ours)]  # Use the same colors as scatter plots\n",
    "for pc, color in zip(violin_parts['bodies'], colors):\n",
    "    pc.set_facecolor(color)\n",
    "    pc.set_edgecolor('black')\n",
    "    pc.set_alpha(0.5)\n",
    "    m = np.mean(pc.get_paths()[0].vertices[:, 0])  # Find the center of the violin\n",
    "    pc.get_paths()[0].vertices[:, 0] = np.clip(pc.get_paths()[0].vertices[:, 0], m, np.inf)  # Clip to the right\n",
    "\n",
    "# Calculate means and confidence intervals for both groups\n",
    "mean_control = np.mean(data_control)\n",
    "mean_active = np.mean(data_active)\n",
    "ci_control = np.std(data_control) * 1.96 / np.sqrt(len(data_control))\n",
    "ci_active = np.std(data_active) * 1.96 / np.sqrt(len(data_active))\n",
    "\n",
    "# Set the x-tick labels\n",
    "ax.set_xticks([1, 2])\n",
    "ax.set_xticklabels(['Hoffmann et al.', 'Ours'])\n",
    "\n",
    "# Set y-label\n",
    "ax.set_ylabel('Residuals')\n",
    "\n",
    "# Add grid to the plot\n",
    "ax.grid(True, which='both', axis='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Remove top and right spines\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Add a horizontal line at y=0\n",
    "ax.axhline(y=0, color='black',  linestyle='--', linewidth=0.8)\n",
    "\n",
    "# Set layout to be tight\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"distributions_1.pdf\")\n",
    "#plt.savefig(\"distributions2.pdf\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "wgCrlwNHvjZ9",
    "outputId": "b8fcc84d-ab6f-4064-ae87-0b30fab6fd7f"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def scaling_law(N, D, params):\n",
    "    logA, logB, logE, alpha, beta = params\n",
    "    A, B, E = np.exp([logA, logB, logE])\n",
    "    return E + A/N**alpha + B/D**beta\n",
    "\n",
    "# Your residuals calculation\n",
    "A, B, E, alpha, beta = true_params_unlogged\n",
    "residuals = losses[indices] - scaling_law(N[indices], D[indices], true_params)  # Hoffmann estimates\n",
    "mse = np.mean(residuals**2)\n",
    "\n",
    "residuals_ours = losses[indices] - np.exp(log_sum_exp(*estimated_params, N[indices], D[indices]))\n",
    "mse_ours = np.mean(residuals_ours**2)\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "t_statistic, p_value = stats.ttest_ind(residuals, residuals_ours, equal_var=True)\n",
    "\n",
    "print(f\"T-statistic: {t_statistic}, P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "yERUaQruame4",
    "outputId": "52bb1ae7-67ef-496c-df8b-232eca71c60f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.path import Path\n",
    "\n",
    "# Assuming residuals_2 and residuals_2_ours are numpy arrays with your data\n",
    "# Define the data\n",
    "\n",
    "color_ours = 0.35\n",
    "color_chinchilla = 0.65\n",
    "\n",
    "data_control = residuals  # replace with your actual data for population-matched controls\n",
    "data_active = residuals_ours  # replace with your actual data for musically active cases\n",
    "\n",
    "# Create a new figure\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "# Check for the largest residual and set the y-limit if necessary\n",
    "largest_residual = max(np.max(data_control), np.max(data_active))\n",
    "if largest_residual > 0.6:\n",
    "    ax.set_ylim(-0.15, 0.2)\n",
    "\n",
    "# Scatter plots with a small random noise to x-values for jitter effect\n",
    "scatter_control = np.random.normal(1, 0.04, size=len(data_control))\n",
    "scatter_active = np.random.normal(2, 0.04, size=len(data_active))\n",
    "ax.scatter(scatter_control, data_control, alpha=0.25, color=plt.cm.viridis(color_chinchilla))\n",
    "ax.scatter(scatter_active, data_active, alpha=0.25, color=plt.cm.viridis(color_ours))\n",
    "\n",
    "# Offset for the violin plots to be right of the scatter points\n",
    "violin_offset = 0.1  # Adjust as needed\n",
    "\n",
    "# Create violin plots on the same axis as scatter plots, slightly offset to the right\n",
    "violin_parts = ax.violinplot([data_control, data_active], positions=[1 + violin_offset, 2 + violin_offset],\n",
    "                             widths=0.8, showmeans=False, showextrema=False, showmedians=True)\n",
    "\n",
    "# Make the violin plot one-sided by adjusting its paths\n",
    "colors = [plt.cm.viridis(color_chinchilla), plt.cm.viridis(color_ours)]  # Use the same colors as scatter plots\n",
    "for pc, color in zip(violin_parts['bodies'], colors):\n",
    "    pc.set_facecolor(color)\n",
    "    pc.set_edgecolor('black')\n",
    "    pc.set_alpha(0.5)\n",
    "    m = np.mean(pc.get_paths()[0].vertices[:, 0])  # Find the center of the violin\n",
    "    pc.get_paths()[0].vertices[:, 0] = np.clip(pc.get_paths()[0].vertices[:, 0], m, np.inf)  # Clip to the right\n",
    "\n",
    "# Calculate means and confidence intervals for both groups\n",
    "mean_control = np.mean(data_control)\n",
    "mean_active = np.mean(data_active)\n",
    "ci_control = np.std(data_control) * 1.96 / np.sqrt(len(data_control))\n",
    "ci_active = np.std(data_active) * 1.96 / np.sqrt(len(data_active))\n",
    "\n",
    "# Set the x-tick labels\n",
    "ax.set_xticks([1, 2])\n",
    "ax.set_xticklabels(['Hoffmann et al.', 'Ours'])\n",
    "\n",
    "# Set y-label\n",
    "ax.set_ylabel('Residuals')\n",
    "\n",
    "# Add grid to the plot\n",
    "ax.grid(True, which='both', axis='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Remove top and right spines\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Add a horizontal line at y=0\n",
    "ax.axhline(y=0, color='black',  linestyle='--', linewidth=0.8)\n",
    "\n",
    "# Set layout to be tight\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"distributions_2.pdf\")\n",
    "#plt.savefig(\"distributions2.pdf\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ro3i-cTsspm0"
   },
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzLheLPQtNds"
   },
   "source": [
    "## Likelihood test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "YyqJdJkmt38R"
   },
   "outputs": [],
   "source": [
    "# Define the objective function to be minimized\n",
    "def objective(params, N, D, losses):\n",
    "    a, b, e, alpha, beta, sigma = params\n",
    "    predictions = log_sum_exp(a, b, e, alpha, beta, N, D)\n",
    "    return -np.sum(huber_logpdf(np.log(losses), loc=predictions, scale=np.exp(sigma), delta=1e-3))\n",
    "    # return custom_huber_loss(np.log(losses), predictions, delta=1e-3)\n",
    "\n",
    "def scale_objective(sigma, params, N, D, losses):\n",
    "    a, b, e, alpha, beta = params\n",
    "    predictions = log_sum_exp(a, b, e, alpha, beta, N, D)\n",
    "    return -np.sum(huber_logpdf(np.log(losses), loc=predictions, scale=np.exp(sigma), delta=1e-3))\n",
    "    # return custom_huber_loss(np.log(losses), predictions, delta=1e-3)\n",
    "\n",
    "def constant_term_objective(params, a, b, alpha, beta, N, D, losses):\n",
    "    e, sigma = params\n",
    "    predictions = log_sum_exp(a, b, e, alpha, beta, N, D)\n",
    "    return -np.sum(huber_logpdf(np.log(losses), loc=predictions, scale=np.exp(sigma), delta=1e-3))\n",
    "\n",
    "def huber_loss_objective(params, N, D, losses):\n",
    "    a, b, e, alpha, beta = params\n",
    "    predictions = log_sum_exp(a, b, e, alpha, beta, N, D)\n",
    "    return custom_huber_loss(np.log(losses), predictions, delta=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Ex-J5XYTtKLW"
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "\n",
    "param_list = np.array(param_list)\n",
    "cov_matrix = np.cov(np.transpose(param_list))\n",
    "\n",
    "param_list_untransformed = untransform_params(param_list)\n",
    "cov_matrix_untransformed = np.cov(np.transpose(param_list_untransformed))\n",
    "\n",
    "init_params = list(true_params) + [0]\n",
    "indices = [i for i in range(len(N)) if losses[i] < sorted(losses)[-5]]\n",
    "\n",
    "# estimate parameters\n",
    "result = minimize(objective, init_params, args=(N[indices], D[indices], losses[indices]), method='BFGS', jac=grad(objective), tol=1e-1)\n",
    "\n",
    "estimated_params = result.x[:5]\n",
    "estimated_params_untransformed = untransform_params(estimated_params)\n",
    "\n",
    "indices_with_outliers = [i for i in range(len(N))]\n",
    "\n",
    "# fit scale param for estimated (without outliers)\n",
    "result = minimize(scale_objective, init_params[-1], args=(estimated_params, N[indices], D[indices], losses[indices]), method='BFGS', jac=grad(scale_objective), tol=1e-1)\n",
    "estimated_params_scale_adjusted_no_outliers = list(estimated_params) + [result.x[0]]\n",
    "\n",
    "# fit scale param for estimated (with outliers)\n",
    "result = minimize(scale_objective, init_params[-1], args=(estimated_params, N[indices_with_outliers], D[indices_with_outliers], losses[indices_with_outliers]), method='BFGS', jac=grad(scale_objective), tol=1e-1)\n",
    "estimated_params_scale_adjusted_with_outliers = list(estimated_params) + [result.x[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "BHNctMDYmfrx"
   },
   "outputs": [],
   "source": [
    "# fit scale param for rounded Chinchilla parameters (without outliers)\n",
    "result = minimize(scale_objective, init_params[-1], args=(true_params_rounded, N[indices], D[indices], losses[indices]), \\\n",
    "                  method='BFGS', jac=grad(scale_objective), tol=1e-9)\n",
    "\n",
    "rounded_chinchilla_params_with_scale_no_outliers = list(true_params_rounded) + [result.x[0]]\n",
    "\n",
    "\n",
    "# fit scale param for rounded Chinchilla parameters (with outliers)\n",
    "result = minimize(scale_objective, init_params[-1], args=(true_params_rounded, N[indices_with_outliers], D[indices_with_outliers], losses[indices_with_outliers]), \\\n",
    "                  method='BFGS', jac=grad(scale_objective), tol=1e-9)\n",
    "\n",
    "rounded_chinchilla_params_with_scale_with_outliers = list(true_params_rounded) + [result.x[0]]\n",
    "\n",
    "\n",
    "# fit scale param for unrounded Chinchilla parameters (with outliers)\n",
    "result = minimize(scale_objective, init_params[-1], args=(true_params, N[indices_with_outliers], D[indices_with_outliers], losses[indices_with_outliers]), method='BFGS', jac=grad(scale_objective), tol=1e-1)\n",
    "\n",
    "unrounded_chinchilla_params_with_scale_with_outliers = list(true_params) + [result.x[0]]\n",
    "\n",
    "\n",
    "# fit scale param for unrounded Chinchilla parameters (with outliers)\n",
    "result = minimize(scale_objective, init_params[-1], args=(true_params, N[indices], D[indices], losses[indices]), method='BFGS', jac=grad(scale_objective), tol=1e-1)\n",
    "\n",
    "unrounded_chinchilla_params_with_scale_no_outliers = list(true_params) + [result.x[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "WIJhKqHulyqz",
    "outputId": "cd1456f4-1a1f-4da4-fa69-3c3bb41deb4d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Print the arrays\n",
    "print(\"estimated_params_scale_adjusted_no_outliers:\")\n",
    "print(estimated_params_scale_adjusted_no_outliers)\n",
    "print()\n",
    "\n",
    "print(\"estimated_params_scale_adjusted_with_outliers:\")\n",
    "print(estimated_params_scale_adjusted_with_outliers)\n",
    "print()\n",
    "\n",
    "print(\"rounded_chinchilla_params_with_scale_no_outliers:\")\n",
    "print(rounded_chinchilla_params_with_scale_no_outliers)\n",
    "print()\n",
    "\n",
    "print(\"rounded_chinchilla_params_with_scale_with_outliers:\")\n",
    "print(rounded_chinchilla_params_with_scale_with_outliers)\n",
    "print()\n",
    "\n",
    "print(\"unrounded_chinchilla_params_with_scale_with_outliers:\")\n",
    "print(unrounded_chinchilla_params_with_scale_with_outliers)\n",
    "print()\n",
    "\n",
    "print(\"unrounded_chinchilla_params_with_scale_no_outliers:\")\n",
    "print(unrounded_chinchilla_params_with_scale_no_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eC9znslTjAh0"
   },
   "source": [
    "Log likelihoods (left column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Mh18gJlRre1V",
    "outputId": "033001e1-6113-474d-8d13-f090cdba7397"
   },
   "outputs": [],
   "source": [
    "log_likelihood_chinchilla_rounding_no_outliers = -objective(rounded_chinchilla_params_with_scale_no_outliers, N[indices], D[indices], losses[indices])\n",
    "\n",
    "print(f\"Likelihood ratios (Chichila, rounded, no outliers): {log_likelihood_chinchilla_rounding_no_outliers:.2f}\")\n",
    "\n",
    "log_likelihood_chinchilla_no_outliers = -objective(unrounded_chinchilla_params_with_scale_no_outliers, N[indices], D[indices], losses[indices])\n",
    "\n",
    "print(f\"Likelihood ratios (Chichila, unrounded, no outliers): {log_likelihood_chinchilla_no_outliers:.2f}\")\n",
    "\n",
    "log_likelihood_our_best_fit_no_outliers = -objective(estimated_params_scale_adjusted_no_outliers, N[indices], D[indices], losses[indices])\n",
    "\n",
    "print(f\"Likelihood ratios (our fit, no outliers): {log_likelihood_our_best_fit_no_outliers:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Ev34MHKhspPf",
    "outputId": "68cbc422-cfd8-448b-8fea-7437351a9496"
   },
   "outputs": [],
   "source": [
    "lambda_LR = -2*(log_likelihood_chinchilla_no_outliers - log_likelihood_our_best_fit_no_outliers)\n",
    "lr_test_df = 5 # 6 parameters fitted for best fit - 1 parameter (scale) fit for chinchilla = 5 degrees of freedom\n",
    "\n",
    "lr_test_p_value = chi2.sf(lambda_LR, df=lr_test_df)\n",
    "print(\"Likelihood ratio test statistic: %.2f\\nWilks distribution (chi^2 with df=%d) p-value: %.2e\" % (lambda_LR, lr_test_df, lr_test_p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dil2q4lSrjty"
   },
   "source": [
    "Log likelihoods (right column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "FWxeUobpsnnZ",
    "outputId": "2e8d92d1-1227-4d5b-ea2e-a9fcafda281b"
   },
   "outputs": [],
   "source": [
    "log_likelihood_chinchilla_rounding_with_outliers = -objective(rounded_chinchilla_params_with_scale_with_outliers, N[indices_with_outliers], D[indices_with_outliers], losses[indices_with_outliers])\n",
    "\n",
    "print(f\"Likelihood ratios (Chichila, rounded, no outliers): {log_likelihood_chinchilla_rounding_with_outliers:.2f}\")\n",
    "\n",
    "log_likelihood_chinchilla_with_outliers = -objective(unrounded_chinchilla_params_with_scale_with_outliers, N[indices_with_outliers], D[indices_with_outliers], losses[indices_with_outliers])\n",
    "\n",
    "print(f\"Likelihood ratios (Chichila, unrounded, no outliers): {log_likelihood_chinchilla_with_outliers:.2f}\")\n",
    "\n",
    "log_likelihood_our_best_fit_with_outliers = -objective(estimated_params_scale_adjusted_with_outliers, N[indices_with_outliers], D[indices_with_outliers], losses[indices_with_outliers])\n",
    "\n",
    "print(f\"Likelihood ratios (our fit, no outliers): {log_likelihood_our_best_fit_with_outliers:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "_RB70fBPrXIP",
    "outputId": "9b0cc2c7-d586-4612-890c-d865d71f4cfd"
   },
   "outputs": [],
   "source": [
    "lambda_LR = -2*(log_likelihood_chinchilla_with_outliers - log_likelihood_our_best_fit_with_outliers)\n",
    "lr_test_df = 5 # 6 parameters fitted for best fit - 1 parameter (scale) fit for chinchilla = 5 degrees of freedom\n",
    "\n",
    "lr_test_p_value = chi2.sf(lambda_LR, df=lr_test_df)\n",
    "print(\"Likelihood ratio test statistic: %.2f\\nWilks distribution (chi^2 with df=%d) p-value: %.2e\" % (lambda_LR, lr_test_df, lr_test_p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09geyxL6IoJa"
   },
   "source": [
    "Estimates and standard errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "UnU_B2yHzc_T",
    "outputId": "78df0330-78a3-4bf1-c191-9b567919cb7c"
   },
   "outputs": [],
   "source": [
    "a_low = 0.454\n",
    "a_high = 0.455\n",
    "a_mid = np.mean([a_low, a_high])\n",
    "\n",
    "estimated_params_with_outliers = np.array([6.28204169, 9.51269708, 0.63748901, 0.35286066, 0.45596155])\n",
    "#estimated_params = np.median(param_list, axis=0)\n",
    "standard_errors = np.sqrt(np.diag(cov_matrix[:5, :5]))\n",
    "\n",
    "parameter_labels = [\"A\", \"B\", \"E\", \"alpha\", \"beta\"]\n",
    "print(\"Parameter estimates and their standard errors\\n\")\n",
    "for index, label in enumerate(parameter_labels):\n",
    "  print(\"%s: %.3f (%.3f)\" % (label, estimated_params[index], standard_errors[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ff11hpSvrMKf",
    "outputId": "30702f55-c744-4745-96c4-227cecf405cc"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "\n",
    "def chi_squared_stat(params_1, params_2, cov_matrix):\n",
    "  return np.transpose(params_1 - params_2) @ np.linalg.inv(cov_matrix) @ (params_1 - params_2)\n",
    "\n",
    "print(\"Difference between Hoffmann et al. (2022) params and our params:\", true_params - estimated_params)\n",
    "chi_squared = chi_squared_stat(true_params, estimated_params, cov_matrix[:5, :5])\n",
    "\n",
    "print(\"Implied chi^2 (df=5) test statistic: %.2f\" % (chi_squared))\n",
    "print(\"Implied chi^2 (df=5) p-value: %.2e\" % (chi2.sf(chi_squared, df=5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuDrEe2eZsPb"
   },
   "source": [
    "# How much data is needed to get confidence bands around a and b that are as tight as Hoffmann et al. report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sfAG5nZ1Z1rQ",
    "outputId": "7a282712-8536-4419-e72e-416f8d6b9265"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# Extract alpha and beta from the parameter list\n",
    "alpha_samples = param_list[:, -2]\n",
    "beta_samples = param_list[:, -1]\n",
    "\n",
    "# Calculate the mean values of alpha and beta\n",
    "mean_alpha = np.mean(alpha_samples)\n",
    "mean_beta = np.mean(beta_samples)\n",
    "\n",
    "# Extract the variances and covariance for alpha and beta\n",
    "var_alpha = cov_matrix_untransformed[-2, -2]\n",
    "var_beta = cov_matrix_untransformed[-1, -1]\n",
    "cov_alpha_beta = cov_matrix_untransformed[-2, -1]\n",
    "\n",
    "# Calculate the partial derivatives of g(alpha, beta) = alpha / (alpha + beta)\n",
    "# with respect to alpha and beta, evaluated at the mean values of alpha and beta\n",
    "partial_g_alpha = -mean_beta / (mean_alpha + mean_beta)**2\n",
    "partial_g_beta = mean_alpha / (mean_alpha + mean_beta)**2\n",
    "\n",
    "# Calculate the variance of the ratio using the delta method\n",
    "var_ratio = (partial_g_alpha**2 * var_alpha +\n",
    "             partial_g_beta**2 * var_beta +\n",
    "             2 * partial_g_alpha * partial_g_beta * cov_alpha_beta)\n",
    "\n",
    "# Calculate the standard error of the ratio\n",
    "se_ratio = np.sqrt(var_ratio)\n",
    "\n",
    "# Calculate the width of the 80% confidence interval\n",
    "width_of_80_ci_band_a = 2 * norm.ppf(0.9) * se_ratio\n",
    "\n",
    "# Assuming you want to maintain a fixed standard error (se_ratio) for a different sample size\n",
    "# and you have a desired width for the confidence interval, calculate the required sample size\n",
    "desired_width = 0.001\n",
    "\n",
    "existing_sample_size = len(N[indices])\n",
    "multiple_by_which_n_needs_to_increase = (width_of_80_ci_band_a / desired_width)**2\n",
    "required_n = existing_sample_size*multiple_by_which_n_needs_to_increase\n",
    "\n",
    "print(required_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_xRyrQWMThc"
   },
   "source": [
    "Confirm with bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OI03x07aVdh-",
    "outputId": "c9a303a3-bf3b-4ea5-f1a9-dbecdad69a8a"
   },
   "outputs": [],
   "source": [
    "a_low = 0.454\n",
    "a_high = 0.455\n",
    "a_mid = np.mean([a_low, a_high])\n",
    "\n",
    "estimated_params_with_outliers = np.array([6.28204169, 9.51269708, 0.63748901, 0.35286066, 0.45596155])\n",
    "#estimated_params = np.median(param_list, axis=0)\n",
    "standard_errors = np.sqrt(np.diag(cov_matrix[:5, :5]))\n",
    "standard_errors_untransformed = np.sqrt(np.diag(cov_matrix_untransformed[:5, :5]))\n",
    "\n",
    "b = param_list[:, -2]/(param_list[:, -2] + param_list[:, -1])\n",
    "a = 1-b\n",
    "\n",
    "b_point_estimate = estimated_params[-2]/(estimated_params[-2] + estimated_params[-1])\n",
    "a_point_estimate = 1 - b_point_estimate\n",
    "\n",
    "parameter_labels = [\"A\", \"B\", \"E\", \"alpha\", \"beta\"]\n",
    "print(\"Parameter estimates and their standard errors\\n\")\n",
    "for index, label in enumerate(parameter_labels):\n",
    "  print(\"%s: %.3f (%.3f)\" % (label, estimated_params_untransformed[index], standard_errors_untransformed[index]))\n",
    "\n",
    "print(\"a = beta/(alpha_beta): %.3f (%.3f)\" % (a_point_estimate, np.std(a)))\n",
    "chinchilla_conf_int_width = desired_width\n",
    "\n",
    "a_std_err = np.std(a)\n",
    "a_conf_int_width = np.percentile(a, 90) - np.percentile(a, 10)\n",
    "chinchilla_a_conf_int_width = 1e-3\n",
    "our_sample_size = len(N[indices])\n",
    "required_sample_size = our_sample_size * (a_conf_int_width/chinchilla_a_conf_int_width)**2\n",
    "\n",
    "print(\"\"\"Our sample size is %d, and a has a standard error of %.3f\n",
    "      and a 80%% conf int width of %.3f at this sample size\"\"\" % (our_sample_size, a_std_err, a_conf_int_width))\n",
    "\n",
    "print(\"To reach 80%% conf int width of %.3f, we would need a sample size of %d\" % (chinchilla_conf_int_width, required_sample_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qz7RgRhS2Jqa"
   },
   "source": [
    "### What if they used intermediate losses and clustered standard errors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "s8EjLsCC13bw",
    "outputId": "8687a9a4-bac6-4fc9-c57f-233ca5691cee"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def calculate_new_se(rho, se_original=0.02, G=500, N=500000): #suppose they have 1k loss values per training run\n",
    "    \"\"\"\n",
    "    Calculate the new standard error using all observations and accounting for clustering.\n",
    "    \"\"\"\n",
    "    n = N / G  # Number of observations per group\n",
    "    effective_N = N / (1 + (n - 1) * rho)  # Effective number of independent observations\n",
    "    new_se = se_original * math.sqrt(G / effective_N)\n",
    "    return new_se\n",
    "\n",
    "# Define rho values from 0.05 to 0.5\n",
    "rho_values = np.linspace(0.25, 0.95, 100)\n",
    "new_se_values = [calculate_new_se(rho) for rho in rho_values]\n",
    "confidence_interval_width = [se * 1.282 * 2 for se in new_se_values]\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(rho_values, confidence_interval_width, marker='o', linestyle='-', color='b')\n",
    "plt.title('Standard Errors vs. Intra-group Correlation Coefficient (rho)')\n",
    "plt.xlabel('Intra-group Correlation Coefficient (rho)')\n",
    "plt.ylabel('80% CI interval width')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjsfTvWcX8f0"
   },
   "source": [
    "# Comparing optimal scaling policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nfHLfcui2HR5"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def scaling_law_reducible(N, D, params):\n",
    "  a, b, e, alpha, beta = params\n",
    "  A, B, E = np.exp([a, b, e])\n",
    "\n",
    "  return A/N**alpha + B/D**beta\n",
    "\n",
    "def G(params):\n",
    "  a, b, e, alpha, beta = params\n",
    "  A, B, E = np.exp([a, b, e])\n",
    "\n",
    "  return ((alpha*A)/(beta*B))**(1/(alpha+beta))\n",
    "\n",
    "def compute_optimal_allocation(compute, params):\n",
    "  a, b, e, alpha, beta = params\n",
    "  A, B, E = np.exp([a, b, e])\n",
    "\n",
    "  G = ((alpha*A)/(beta*B))**(1/(alpha+beta))\n",
    "  a = beta/(alpha+beta)\n",
    "  b = 1 - a\n",
    "\n",
    "  return G*(compute/6)**a, G**(-1) * (compute/6)**b\n",
    "\n",
    "def compute_optimal_reducible_loss(compute, params):\n",
    "  N_opt, D_opt = compute_optimal_allocation(compute, params)\n",
    "  return scaling_law_reducible(N_opt, D_opt, params)\n",
    "\n",
    "def optimal_compute_from_reducible_loss(loss, params):\n",
    "  a, b, e, alpha, beta = params\n",
    "  A, B, E = np.exp([a, b, e])\n",
    "\n",
    "  G = ((alpha*A)/(beta*B))**(1/(alpha+beta))\n",
    "  a = beta/(alpha+beta)\n",
    "  b = 1 - a\n",
    "\n",
    "  return 6 * (loss/(G**(-alpha) * A + G**beta * B))**(-(alpha + beta)/(alpha*beta))\n",
    "\n",
    "def compute_optimal_allocation_from_shares(compute, G, a):\n",
    "  b = 1-a\n",
    "  return G*(compute/6)**a, G**(-1) * (compute/6)**b\n",
    "\n",
    "def ratio(params_and_tokens):\n",
    "  params, tokens = params_and_tokens\n",
    "  return tokens/params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQ0H9wf84ime"
   },
   "outputs": [],
   "source": [
    "compute_thresholds = 10**np.arange(18, 28, 0.05)\n",
    "conf_int_percentile = 80\n",
    "low, high = (100-conf_int_percentile)/2, 100 - (100-conf_int_percentile)/2\n",
    "\n",
    "D_N_ratio_conf_int = [[], [], []]\n",
    "D_N_ratios = []\n",
    "chinchilla_D_N_ratio = []\n",
    "\n",
    "compute_loss_factors = []\n",
    "\n",
    "simulated_params_list = multivariate_normal.rvs(mean=estimated_params, cov=cov_matrix[:5, :5], size=10000)\n",
    "\n",
    "for threshold in compute_thresholds:\n",
    "  D_N_ratio = []\n",
    "  compute_loss_factor = []\n",
    "\n",
    "  N_true_opt, D_true_opt = compute_optimal_allocation_from_shares(threshold, G(true_params), a_mid)\n",
    "  D_N_true_ratio = D_true_opt/N_true_opt\n",
    "\n",
    "  for simulated_params in simulated_params_list:\n",
    "    N_opt, D_opt = compute_optimal_allocation(threshold, simulated_params)\n",
    "    D_N_ratio.append(D_opt/N_opt)\n",
    "\n",
    "    loss_achieved_by_chinchilla = scaling_law_reducible(N_true_opt, D_true_opt, simulated_params)\n",
    "    compute_needed_for_loss = optimal_compute_from_reducible_loss(loss_achieved_by_chinchilla, simulated_params)\n",
    "\n",
    "    compute_loss_factor.append(threshold/compute_needed_for_loss)\n",
    "\n",
    "  D_N_ratio_conf_int[0].append(np.percentile(D_N_ratio, low))\n",
    "  D_N_ratio_conf_int[1].append(np.median(D_N_ratio))\n",
    "  D_N_ratio_conf_int[2].append(np.percentile(D_N_ratio, high))\n",
    "\n",
    "  chinchilla_D_N_ratio.append(D_N_true_ratio)\n",
    "\n",
    "  D_N_ratios.append(D_N_ratio)\n",
    "  compute_loss_factors.append(compute_loss_factor)\n",
    "\n",
    "D_N_ratios = np.array(D_N_ratios)\n",
    "compute_loss_factors = np.array(compute_loss_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F6D5ql9_AYMh"
   },
   "outputs": [],
   "source": [
    "def log_format(val, pos):\n",
    "    \"\"\"Format the tick labels on logarithmic scale.\"\"\"\n",
    "    val_str = '{:g}'.format(val)\n",
    "    if float(val_str) >= 1.0:\n",
    "        # If the value is a whole number, return it as an integer.\n",
    "        return str(int(val))\n",
    "    else:\n",
    "        # Otherwise, return the string as is (useful for fractional values).\n",
    "        return val_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HYL1DzjOj_tK"
   },
   "outputs": [],
   "source": [
    "# Assuming your previous variables and data (compute_thresholds, D_N_ratio_conf_int, etc.) are defined\n",
    "chinchilla_lower = ratio(compute_optimal_allocation_from_shares(compute_thresholds, G(true_params), a_low))\n",
    "chinchilla_upper = ratio(compute_optimal_allocation_from_shares(compute_thresholds, G(true_params), a_high))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "ea0DZ1y9ksUI",
    "outputId": "e39e741f-31dc-4ada-b02a-d3488b8f3006"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "chinchilla_compute = (1.4*10**12)*(70*10**9)*6\n",
    "\n",
    "color_ours = 0.35\n",
    "color_chinchilla = 0.65\n",
    "\n",
    "# Define line width for better visibility\n",
    "line_width = 2.5\n",
    "\n",
    "plt.figure(figsize=(15/2, 7/1.85))  # Width: 10 inches, Height: 6 inches\n",
    "\n",
    "plt.plot(compute_thresholds, D_N_ratio_conf_int[1], label=\"Optimal policy (ours)\", color=plt.cm.viridis(color_ours), linewidth=line_width)\n",
    "#plt.plot(compute_thresholds, D_N_ratio_conf_int[0], color=plt.cm.viridis(color_ours), linestyle=\"dashed\")\n",
    "#plt.plot(compute_thresholds, D_N_ratio_conf_int[2], color=plt.cm.viridis(color_ours), linestyle=\"dashed\")\n",
    "\n",
    "plt.plot(compute_thresholds, chinchilla_D_N_ratio, \\\n",
    "         label=\"Optimal policy (Hoffmann et al.)\", color=plt.cm.viridis(color_chinchilla), linewidth=line_width)\n",
    "# Assuming chinchilla_lower and chinchilla_upper are defined previously along with their respective function calculations\n",
    "#plt.plot(compute_thresholds, chinchilla_lower, label=\"\", color=plt.cm.viridis(color_chinchilla), linestyle=\"dashed\")\n",
    "#plt.plot(compute_thresholds, chinchilla_upper, label=\"\", color=plt.cm.viridis(color_chinchilla), linestyle=\"dashed\")\n",
    "\n",
    "plt.fill_between(compute_thresholds, D_N_ratio_conf_int[0], D_N_ratio_conf_int[2], color=plt.cm.viridis(color_ours), alpha=0.25, label='')\n",
    "plt.fill_between(compute_thresholds, chinchilla_lower, chinchilla_upper, color=plt.cm.viridis(color_chinchilla), alpha=0.25, label='')\n",
    "\n",
    "plt.axhline(y=20, color='gray', linestyle='--')\n",
    "plt.text(x=7e24, y=20, s=r\"$D/N = 20$ rule of thumb\", color='gray', fontsize = 10, verticalalignment='bottom')\n",
    "\n",
    "# Adding the round filled marker at (chinchilla_compute, 20)\n",
    "plt.plot(chinchilla_compute, 20, 'o', color='black', markersize=8, label=\"Chinchilla model\", alpha=0.75)  # 'o' is the marker style for a filled circle\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.gca().yaxis.set_major_formatter(FuncFormatter(log_format))\n",
    "\n",
    "plt.xlim([min(compute_thresholds), max(compute_thresholds)])\n",
    "plt.subplots_adjust(bottom=0.15)  # Adjust bottom margin to make space for the x-axis\n",
    "\n",
    "plt.xlabel(\"Training compute (FLOP)\")\n",
    "plt.ylabel(\"Tokens per parameters ratio\")\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.savefig(\"tokens_to_params_ratio_plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G4TZyNoQR2lP",
    "outputId": "2a1e8407-f47a-471b-bcb5-2785c667946a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming compute_thresholds and chinchilla_D_N_ratio are numpy arrays or can be converted into numpy arrays\n",
    "compute_thresholds = np.array(compute_thresholds)  # Convert to numpy array if not already\n",
    "chinchilla_D_N_ratio = np.array(chinchilla_D_N_ratio)  # Convert to numpy array if not already\n",
    "\n",
    "# Calculate the absolute difference between each element in compute_thresholds and chinchilla_compute\n",
    "abs_difference = np.abs(compute_thresholds - chinchilla_compute)\n",
    "\n",
    "# Find the index of the smallest difference\n",
    "index_closest = np.argmin(abs_difference)\n",
    "\n",
    "# Retrieve and print the value of chinchilla_D_N_ratio at this index\n",
    "value_closest = chinchilla_D_N_ratio[index_closest]\n",
    "print(f\"The value of chinchilla_D_N_ratio closest to chinchilla_compute is: {value_closest}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZfLjrFlx-amc",
    "outputId": "4cb8b5c1-5a42-4ca7-b577-1050fef0196e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming compute_thresholds and chinchilla_D_N_ratio are numpy arrays or can be converted into numpy arrays\n",
    "compute_thresholds = np.array(D_N_ratio_conf_int[1])  # Convert to numpy array if not already\n",
    "chinchilla_D_N_ratio = np.array(D_N_ratio_conf_int[1])  # Convert to numpy array if not already\n",
    "\n",
    "# Calculate the absolute difference between each element in compute_thresholds and chinchilla_compute\n",
    "abs_difference = np.abs(compute_thresholds - chinchilla_compute)\n",
    "\n",
    "# Find the index of the smallest difference\n",
    "index_closest = np.argmin(abs_difference)\n",
    "\n",
    "# Retrieve and print the value of chinchilla_D_N_ratio at this index\n",
    "value_closest = chinchilla_D_N_ratio[index_closest]\n",
    "print(f\"The value of chinchilla_D_N_ratio closest to chinchilla_compute is: {value_closest}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dau4oi_oXg8W"
   },
   "source": [
    "# Replicate with lower loss scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AK-Dj7iNXwPm"
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd.scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import erf\n",
    "\n",
    "true_params = np.array([np.log(406.4), np.log(410.7), np.log(1.69), 0.34, 0.28])\n",
    "\n",
    "# Define the log-sum-exp function\n",
    "def log_sum_exp(a, b, e, alpha, beta, N, D):\n",
    "    return np.log(np.exp(a - alpha * np.log(N)) + np.exp(b - beta * np.log(D)) + np.exp(e))\n",
    "\n",
    "# Define the Huber loss function\n",
    "def custom_huber_loss(y_true, y_pred, delta=1e-3, reduce_fn=np.sum):\n",
    "    # Calculate the difference\n",
    "    diff = y_true - y_pred\n",
    "    # Calculate the condition for Huber loss\n",
    "    cond = np.abs(diff) <= delta\n",
    "    # Apply Huber loss formula\n",
    "    loss = np.where(cond, 0.5 * diff**2, delta * (np.abs(diff) - 0.5 * delta))\n",
    "    return reduce_fn(loss)\n",
    "\n",
    "def huber_normalizing_factor(delta=1e-3):\n",
    "    return np.sqrt(2*np.pi) * (1 - 2*norm.sf(delta)) + 2 * np.exp(-0.5*delta**2)/delta\n",
    "\n",
    "def huber_logpdf(x, delta=1e-3, loc=0, scale=1):\n",
    "    x = (x-loc)/scale\n",
    "\n",
    "    cond = np.abs(x) <= delta\n",
    "    loss = np.where(cond, 0.5 * x**2, delta * (np.abs(x) - 0.5 * delta))\n",
    "    return -loss - np.log(huber_normalizing_factor(delta=delta)) - np.log(scale)\n",
    "\n",
    "def huber_pdf(x, delta=1e-3, loc=0, scale=1):\n",
    "    return np.exp(huber_logpdf(x, delta=delta, loc=loc, scale=scale))\n",
    "\n",
    "# Define the objective function to be minimized\n",
    "def objective(params, N, D, losses):\n",
    "    a, b, e, alpha, beta, sigma = params\n",
    "    predictions = log_sum_exp(a, b, e, alpha, beta, N, D)\n",
    "    return -np.sum(huber_logpdf(np.log(losses), loc=predictions, scale=np.exp(sigma), delta=1e-3))\n",
    "    # return custom_huber_loss(np.log(losses), predictions, delta=1e-3)\n",
    "\n",
    "def scale_objective(sigma, params, N, D, losses):\n",
    "    a, b, e, alpha, beta = params\n",
    "    predictions = log_sum_exp(a, b, e, alpha, beta, N, D)\n",
    "    return -np.sum(huber_logpdf(np.log(losses), loc=predictions, scale=np.exp(sigma), delta=1e-3))\n",
    "    # return custom_huber_loss(np.log(losses), predictions, delta=1e-3)\n",
    "\n",
    "def constant_term_objective(params, a, b, alpha, beta, N, D, losses):\n",
    "    e, sigma = params\n",
    "    predictions = log_sum_exp(a, b, e, alpha, beta, N, D)\n",
    "    return -np.sum(huber_logpdf(np.log(losses), loc=predictions, scale=np.exp(sigma), delta=1e-3))\n",
    "\n",
    "def huber_loss_objective(params, N, D, losses, reduce_fn=np.sum):\n",
    "    a, b, e, alpha, beta = params\n",
    "    predictions = log_sum_exp(a, b, e, alpha, beta, N, D)\n",
    "    return custom_huber_loss(np.log(losses), predictions, delta=1e-3, reduce_fn=reduce_fn)\n",
    "\n",
    "# Define the parameter untransform\n",
    "def untransform_params(param_array):\n",
    "    if len(np.shape(param_array)) == 2:\n",
    "      return np.hstack((np.exp(param_array[:, :3]), param_array[:, 3:]))\n",
    "    else:\n",
    "      return np.hstack((np.exp(param_array[:3]), param_array[3:]))\n",
    "\n",
    "# Define the Huber loss function on residuals\n",
    "def huber_loss(residuals, delta=1e-3):\n",
    "    # Calculate the difference\n",
    "    diff = residuals\n",
    "    # Calculate the condition for Huber loss\n",
    "    cond = np.abs(diff) <= delta\n",
    "    # Apply Huber loss formula\n",
    "    loss = np.where(cond, 0.5 * diff**2, delta * (np.abs(diff) - 0.5 * delta))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iwbRuhmvXjxF",
    "outputId": "f5628958-efdd-48c2-ce69-7e727b653a47"
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "# Set up the grid for initial parameter values\n",
    "alpha_vals = np.linspace(0., 2., 10)\n",
    "beta_vals = np.arange(0, 2.5, 0.5)\n",
    "e_vals = np.linspace(0., 2., 10)\n",
    "a_vals = np.linspace(6, 30, 10)\n",
    "b_vals = np.arange(0, 30, 5)\n",
    "\n",
    "# Perform the optimization using L-BFGS over the grid of initial values\n",
    "best_loss = np.inf\n",
    "best_params = None\n",
    "\n",
    "from itertools import product\n",
    "results_dict = {}\n",
    "for alpha, e, a in product(alpha_vals, e_vals, a_vals):\n",
    "    init_params = [a, a, e, alpha, alpha]\n",
    "    result = minimize(functools.partial(huber_loss_objective, reduce_fn=np.sum),\n",
    "                      init_params, args=(N[indices], D[indices], losses[indices]),\n",
    "                      method='L-BFGS-B')\n",
    "    # print(result.message)\n",
    "    results_dict[tuple(init_params)] = {'params': result.x, 'loss': result.fun}\n",
    "    if result.success and result.fun < best_loss:\n",
    "        best_loss = result.fun\n",
    "        best_params = result.x\n",
    "        print(f\"New best loss: {best_loss}\")\n",
    "        print(f\"Best params: {best_params}\")\n",
    "        print(f\"Initial guess: {init_params}\")\n",
    "\n",
    "# Transform the fitted parameters a, b, e to A, B, E\n",
    "if best_params is not None:\n",
    "    A = np.exp(best_params[0])\n",
    "    B = np.exp(best_params[1])\n",
    "    E = np.exp(best_params[2])\n",
    "    alpha = best_params[3]\n",
    "    beta = best_params[4]\n",
    "    print(f\"Best fit parameters: A={A}, B={B}, E={E}, alpha={alpha}, beta={beta}\")\n",
    "else:\n",
    "    print(\"Optimization failed to converge.\")\n",
    "\n",
    "A = np.exp(true_params[0])\n",
    "B = np.exp(true_params[1])\n",
    "E = np.exp(true_params[2])\n",
    "alpha = true_params[3]\n",
    "beta = true_params[4]\n",
    "print(f\"\\nParameters from Chinchilla paper: A={A}, B={B}, E={E}, alpha={alpha}, beta={beta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MeKh2Ezjh8Wh"
   },
   "source": [
    "Show effect of lower loss scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ip1gBmd_h_NP"
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd.scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import erf\n",
    "\n",
    "# true_params = np.array([np.log(406.4), np.log(410.7), np.log(1.69), 0.34, 0.28])\n",
    "true_params = np.array([6.0073404, 6.0179186, 0.5267228, 0.33917084, 0.2849083])\n",
    "true_params_rounded = np.array([np.log(406.4), np.log(410.7), np.log(1.69), 0.34, 0.28])\n",
    "\n",
    "# Define the log-sum-exp function\n",
    "def log_sum_exp(a, b, e, alpha, beta, N, D):\n",
    "    return np.log(np.exp(a - alpha * np.log(N)) + np.exp(b - beta * np.log(D)) + np.exp(e))\n",
    "\n",
    "# Define the Huber loss function\n",
    "def custom_huber_loss_sum(y_true, y_pred, delta=1e-3):\n",
    "    # Calculate the difference\n",
    "    diff = y_true - y_pred\n",
    "    # Calculate the condition for Huber loss\n",
    "    cond = np.abs(diff) <= delta\n",
    "    # Apply Huber loss formula\n",
    "    loss = np.where(cond, 0.5 * diff**2, delta * (np.abs(diff) - 0.5 * delta))\n",
    "    return np.sum(loss)\n",
    "\n",
    "def huber_loss_objective_sum(params, N, D, losses):\n",
    "    a, b, e, alpha, beta = params\n",
    "    predictions = log_sum_exp(a, b, e, alpha, beta, N, D)\n",
    "    return custom_huber_loss_sum(np.log(losses), predictions, delta=1e-3)\n",
    "\n",
    "# Define the Huber loss function\n",
    "def custom_huber_loss_mean(y_true, y_pred, delta=1e-3):\n",
    "    # Calculate the difference\n",
    "    diff = y_true - y_pred\n",
    "    # Calculate the condition for Huber loss\n",
    "    cond = np.abs(diff) <= delta\n",
    "    # Apply Huber loss formula\n",
    "    loss = np.where(cond, 0.5 * diff**2, delta * (np.abs(diff) - 0.5 * delta))\n",
    "    return np.mean(loss)\n",
    "\n",
    "def huber_loss_objective_mean(params, N, D, losses):\n",
    "    a, b, e, alpha, beta = params\n",
    "    predictions = log_sum_exp(a, b, e, alpha, beta, N, D)\n",
    "    return custom_huber_loss_mean(np.log(losses), predictions, delta=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "WI0QC5RQdwUM",
    "outputId": "941340cb-6027-4c6c-9fe8-269a34ff0ae4"
   },
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "init_params = list(true_params)\n",
    "\n",
    "# Store loss values\n",
    "sum_loss_values_for_sum_opt = []\n",
    "sum_loss_values_for_mean_opt = []\n",
    "\n",
    "# Callback functions to log loss values\n",
    "def callback_sum(params):\n",
    "    sum_loss = huber_loss_objective_sum(params, N[indices], D[indices], losses[indices])\n",
    "    sum_loss_values_for_sum_opt.append(sum_loss)\n",
    "\n",
    "def callback_mean(params):\n",
    "    # Calculate the sum loss even though the mean loss is being optimized\n",
    "    sum_loss = huber_loss_objective_sum(params, N[indices], D[indices], losses[indices])\n",
    "    sum_loss_values_for_mean_opt.append(sum_loss)\n",
    "\n",
    "# Perform optimization with logging\n",
    "result_sum = minimize(huber_loss_objective_sum, init_params, args=(N[indices], D[indices], losses[indices]),\n",
    "                      jac=grad(huber_loss_objective_sum), method='BFGS', callback=callback_sum)\n",
    "\n",
    "init_params = list(true_params)\n",
    "result_mean = minimize(huber_loss_objective_mean, init_params, args=(N[indices], D[indices], losses[indices]),\n",
    "                       method='L-BFGS-B', callback=callback_mean)\n",
    "\n",
    "# Plotting the loss values\n",
    "plt.plot(sum_loss_values_for_sum_opt, label='Loss scale (corrected)')\n",
    "plt.plot(sum_loss_values_for_mean_opt, label='Loss scale (Hoffmann et al.)')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "#plt.title('Sum Loss vs. Iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JxtPMLfij4tX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "REwsunoSi2qZ",
    "_9613JmZi4rL",
    "AuDrEe2eZsPb",
    "dau4oi_oXg8W"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ChinchReplic_Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
